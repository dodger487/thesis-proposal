% Motivation, what this section is about
% Data collection
% Labeling schemes (include table)
% Face: Calibration plot and face to accuracy
% Location: Calibration plot and face to accuracy
% Scaling: Agreement plot
% Debiasing
% Conclusion.


% Motivation, what this section is about
In this section, we evaluate the feasibility of combining publicly available social media images and metadata (including location) with modern facial recognition techniques to conduct large scale demographic research.
We find that facial recognition can be used to label the gender and race of social media profiles with high precision and recall.
We further investigate factors that improve or hinder demographic labeling accuracy, showing a disparity between the accuracy of labelings of profiles of racial majority and minorities.
% We conclude with ideas for future improvements and research.
% \end{abstract}

% Data collection
% Fig: Table of data
\begin{table}[h]
\centering
\begin{tabular}{l r r r r}
\emph{Name} & \emph{Users} & \emph{Photos} & \emph{Geotagged} & \emph{Prosograms} \\ \hline
Full & 260,389 & 115M & 16.5M & \\ 
Face & 4,166 & 1.5M & 64.6k & 322k \\ 
Labeled & 172 & 16,655 & 5,489 & 5,272 \\ 
\end{tabular}
\caption{Overview of Datasets \label{tab:data}}
\end{table}

For our research, we used a large dataset of Instagram photo metadata obtained through Instagram's API under the Terms of Service.
Similar to the work described in~\chap{sec:bias}, we gathered the metadata (such as time of photo, URL of image, tags, location, etc.) for all photographs of a ``root" user, Kevin Systrom, the founder of Instagram, and then collected the user IDs of users who had commented on or ``liked" his photos, gathered their metadata, and repeating this process in a random outward searching manner.
This process resulted in over 115 million photos on over 260,000 profiles.
A summary of this data (and derived data described in subsequent sections) is displayed in Table~\ref{tab:data}.

For subsets of profiles, we gathered additional information.
For all photos, we included geographic information when available.
For a randomly selected subset, we ran a publicly available face recognition API\footnote{\url{faceplusplus.com}} on a users first 100 Instagram images.
In addition to recognizing faces within images, Face++ labels race from \{White, Black, Asian\} with a \emph{confidence score} 0-100 and gender from \{Male, Female\} with a \emph{confidence score} 0-100.
For a smaller subset of users, two research assistants labeled a randomly selected subset for the collected profiles for gender and race, with 98.8\% agreement on gender and 85.5\% agreement on race, with the labels consistent with the Face++ set.


% Labeling schemes (include table)
% Fig: Table of labeling techniques
\begin{table}[h]
\centering
\begin{tabular}{l l l l}
\emph{Name} & \emph{Cost} & \emph{Speed} & \emph{Accuracy} \\ \hline
Manual & Expensive & Moderate & High \\ 
Face Recognition & Free / Cheap & Slow & Moderate \\ 
Census & Free & Fast & Low \\ 
\end{tabular}
\caption{Demographic Labeling Techniques \label{tab:techniques}}
\end{table}

There are many different ways to go from the ``raw" signal of social media to clean labels of demographics.
In this section, we consider three, with the relative comparison of the costs and benefits displayed in Table~\ref{tab:techniques}.
\textbf{Manually} labeling profiles using humans to examine each one will result in the highest accuracy, but will have high monetary costs.
Time for labeling can be moderate if done in parallel with many labels, and much slower if done serially by few individuals. 
\textbf{Face recognition} can provide a valuable signal for gender or race based on image analysis.
Depending on the implementation, this can be very cheap, as it is bounded by computation.
However, face recognition algorithms are imperfect and may exhibit statistical bias towards different groups, and may be slow if bounded by API services or computation.
Finally, using \textbf{location data}, as described in \chap{sec:bias}, can be incredibly fast and cheap, but will have low accuracy depending on prediction task, location granularity, and other factors.
We next explore in detail the accuracy of each of these methods.
For the sake of brevity, these results include only information on predicting US Census Race classifications, but we have additional work on binray gender classification available as well.

% Location: Calibration plot and face to accuracy
% Fig: confusion matrix
% \begin{wrapfigure}{R}{0.5\textwidth}
\begin{table}[h]
\centering
\label{tab:cm_race_loc}
\begin{tabular}{l r r}
\emph{Race} & \emph{Predicted Minority} & \emph{Predicted White} \\ \hline
Minority & 8 & 31 \\
White & 5 & 88 \\
\end{tabular}
\caption{Location-Based Race Labeling Confusion Matrix}
\end{table}
% \end{wrapfigure}

\begin{wrapfigure}{R}{0.48\textwidth}
% \begin{figure}[h]
  \centering
  % \begin{minipage}{\halfwidth}
    % \centering
    \includegraphics[width=0.48\linewidth]{fig/census/calibration_race_loc-eps-converted-to.pdf}
  % \end{minipage}
  % \begin{minipage}{\halfwidth}
    % \centering
    \includegraphics[width=0.48\linewidth]{fig/census/locs_v_accuracy_race_labeled-eps-converted-to.pdf}
  % \end{minipage}
  \caption{Location Prediction: Calibration and data amount to accuracy of race prediction.\label{fig:accuracy_race_loc}}
% \end{figure}
\end{wrapfigure}

Based on previous results, we expected location to provide more of a signal for race than for gender.
Census tracts in the United States show a much more skewed distribution in regards to race than to gender, with the majority of tracts being heavily Caucasian, but with a sizeable fraction of tracts (>6\%) having smaller than 20\% Caucasian. 
On a baseline of 70.4\% white users (including Hispanics), using the average percentage of census tracts visited predicted 73.4\% of users' races.
Our figures show that there is some weak signal in terms of ROC.
The algorithm is well calibrated but again does not make strong predictions on most users.
Accuracy is very poor on minorities, compared to Whites, and after an initial jump in accuracy after ten locations, is not greatly affected by the number of geotagged in the users profile.
The number of locations used seems to have little effect on accuracy, which remains low for minorities and high for Whites.


% Face: Calibration plot and face to accuracy

\begin{table}[h]
\centering
\begin{tabular}{l r r}
\emph{Race} & \emph{Labeled Minority} & \emph{Labeled White} \\ \hline
Minority & 33 & 11 \\
White & 4 & 97 \\
\end{tabular}
\caption{Content-Based Race Labeling Confusion Matrix\label{tab:cm_race_face}}
\end{table}

\begin{wrapfigure}{R}{0.5\textwidth}
% \begin{figure}[h]
  \centering
  % \begin{minipage}{\halfwidth}
    % \centering
    \includegraphics[width=0.48\linewidth]{fig/census/calibration_race_face-eps-converted-to.pdf}
  % \end{minipage}
  % \begin{minipage}{\halfwidth}
    % \centering
    \includegraphics[width=0.48\linewidth]{fig/census/faces_v_accuracy_race_labeled-eps-converted-to.pdf}
  % \end{minipage}
  \caption{Face prediction: Calibration and data amount to accuracy of race prediction.\label{fig:accuracy_race_face}}
% \end{figure}
\end{wrapfigure}

In Fig.~\ref{fig:accuracy_race_face}, we see that the algorithm is underconfident, outputting a lower probability than warranted when predicting if a user is white.
An important aspect of demographic labeling is considering issues of the digital divide or disparate impact.
In the rightmost plot of Fig.~\ref{fig:accuracy_race_face}, we see that accuracy is much lower on minorities than it is on White users.
Errors seem biased towards underrepresentation of minority groups.
This could have consequences as the algorithmics introduce a hegemonic factor in favor the majority.
% In section~\ref{sec:scaling} we discuss a principled technique for maintaining equal accuracy rates across groups while maintaining high overall accuracy.

% Scaling: Agreement plot
% \begin{figure}[h]
%   \centering
%   \begin{minipage}{\halfwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/census/faces_v_accuracy_gender_large-eps-converted-to.pdf}
%   \end{minipage}
%   \begin{minipage}{\halfwidth}
\begin{wrapfigure}{R}{0.33\textwidth}
  \centering
  \includegraphics[width=\linewidth]{fig/census/faces_v_accuracy_race_large-eps-converted-to.pdf}
  \caption{Agreement as data increases}
  % \end{minipage}
  \label{fig:faces_v_accuracy_large}
\end{wrapfigure}
% \end{figure}

In both our face recognition and location-based labeling methods, as the number of photos used increases, the impact of one individual photo goes down.
We therefore should expect that at some point we should observe ``diminishing returns" in terms of the amount of data collected.
In other words, we do not need to crawl all photos of a profile in order to know what our algorithm will label it.
We investigate this idea in Figure~\ref{fig:faces_v_accuracy_large}.

We first gather all face data for each user, labeling their entire profile.
Using all of the data, we a label a profile for gender and race using our algorithms.
For each user, we then restrict the data input to the algorithm to a smaller subset and record our prediction.
We repeat this process, gradually decreasing the amount of data supplied to the algorithm until we only give one photo.
For each number of prosograms used, we calculated what percentage of users have the same label as their label from the full labeling.
We see in the left figure that if we used just 1 picture, the output of our algorithm would agree with the full data about 75\% of the time, and this is the same for both male and female profiles.
After 100 prosograms, very few profiles will change their label.
This suggests that for gender, we need only collect the first 100 prosograms to get a highly accurate estimate on our algorithm's result on the entire user profile data.

We observe similar behavior in the race dataset, depicted in the right side of Fig~\ref{fig:faces_v_accuracy_large}.
After 100 photos, our algorithm is similarly unlikely to change its label.
However, in contrast to gender, we see divergent levels of agreement for our two labels, with profiles eventually labeled minority more likely to disagree than white profiles.
This points to a potential bias in either the data or in the labeling algorithm, again suggesting that care must be taken in order to achieve balanced error rates across classifications.

% Debiasing
\paragraph{Debiasing}
Oftentimes, scientists might choose to optimize accuracy or a class-centric metric like F1.
However, choosing to optimize for metrics focused on one class (such as Female or White) can cause systemic biases to appear, as accuracy improves for one class but is ignored for another.
This can be of particular in demographic labeling, where we would like to have proper representation of multiple groups.
For example, when using the threshold of 0.5, we see large disparities between White and Minority labels as the data scales, as appears in Figures~\ref{fig:accuracy_race_loc} and~\ref{fig:accuracy_race_face}.

\begin{wrapfigure}{R}{0.48\textwidth}
  \centering
    \includegraphics[width=0.48\linewidth]{fig/census/locs_v_accuracy_race_labeled_ber-eps-converted-to.pdf}
    \includegraphics[width=0.48\linewidth]{fig/census/faces_v_accuracy_race_labeled_ber-eps-converted-to.pdf}
  \caption{Location or faces vs. accuracy, with a choice of threshold determine by Balanced Error Rate.\label{fig:debias}}
\end{wrapfigure}

% \begin{wrapfigure}[R]{0.5\textwidth}
%   \centering
% % \begin{figure}[h]
%   % \begin{minipage}{\halfwidth}
%     % \centering
%     \includegraphics[width=0.48\linewidth]{fig/census/locs_v_accuracy_race_labeled_ber-eps-converted-to.pdf}
%   % \end{minipage}
%   % \begin{minipage}{\halfwidth}
%     % \centering
%     \includegraphics[width=0.48\linewidth]{fig/census/faces_v_accuracy_race_labeled_ber-eps-converted-to.pdf}
%   % \end{minipage}
%   \caption{Location or faces vs. accuracy, with a choice of threshold determine by Balanced Error Rate.\label{fig:debias}}
% % \end{figure}
% \end{wrapfigure}

Instead of optimizing for accuracy or F1, one method to insure equal levels of error in both classes is to instead optimize for Balanced Error Rate (BER).
Balanced error rate is simply an average of the error in each class, that is, 
$\frac{1}{2}\epsilon_1 + \frac{1}{2}\epsilon_2$, where $\epsilon_1$ and $\epsilon_2$ represent the error in class 1 and 2 (e.g. Female and Male, or White and Minority).

% Interpret results
Figure~\ref{fig:debias} shows how accuracy scales with more data for race when using a threshold determined by optimal BER. 
The thresholds were 0.58 for faces and 0.67 for location.
Comparing with Figures~\ref{fig:faces_v_accuracy_race_labeled} and~\ref{fig:locs_v_accuracy_race_labeled_1}, we see lines much closer together, indicating error rates much closer to one another than previously.
Choosing another metric, such as F1, indeed leads to problems in the example of location.
Here, a threshold of 0.33 is chosen.
Although this gives an accuracy of 100\% for White, it affords only 10.3\% accuracy for minorities.

Using BER as a metric in training can help us in designing algorithms that will fairly label users when conducting demographic research.
Such techniques are especially important when being used in unsupervised settings, where algorithmic bias can impact accuracy on a large scale.


% Conclusion...





% \textbf{Introduction} \\
% The great wealth of publicly available, online social networking data has been a boon to demographic research due to its richness and scale.
% Never before has such an amount of human behavioral data been easily obtained and analyzed.
% However, before this data can be used to study demographics, each user must be labeled with demographic categorizations. 
% This poses a particular challenge in many online social networking (OSN) sites which often do not display or even obtain the demographic information of its users.

% To meet this need, researchers in the past have tried a variety of techniques.
% Manual labeling by individually investigating each profile is costly in terms of time, effort, and money.
% Some studies have relied on data provided by marketing companies or data aggregators~\cite{Goel:2012ut, biinferring}.
% Due to cost and issues of reproducibility, these sources of data are not available to all researchers.

% To improve the scale of labeling while keeping costs low, researchers have used automated techniques which range in complexity.
% For example, researchers have compared public names to lists of gender and ethnicity for those names~\cite{mislove-2011-twitter, ICWSM101534}.
% Others have run simple algorithms on location data~\cite{riederer2015cosn} or more sophisticated techniques that incorporate text posted and the structure of a user's social network~\cite{ICWSM112886, pennacchiotti2011democrats}.
% These techniques offer some promise but often are not very robust and may not be applicable to OSNs with little textual interaction, such as Instagram.

% % TODO: This doesn't really fit.
% Researchers that use these automated tools for labeling must be wary of introducing algorithmic bias.
% As argued in~\cite{Selbst:2014wi}, data mining can lead to biased results.
% Algorithms that have disparate accuracies in demographic labeling could cause erroneous or biased results.

% In the past, computational vision has not been an effective technique for labeling the demographics of social media users, due to three issues: (1) CV algorithms being too slow, (2) CV algorithms having low accuracy, (3) a lack of publicly available and uniformly popular photographs as data.
% However, in recent years, face recognition tools have improved to become both highly accurate and efficient.
% Additionally, the social network Instagram has made image-sharing a ubiquitous activity across most of the developed and much of the developing world.

% Instagram is an interesting social network to study for a variety of reasons.
% With over 400 million users at the time of writing, over 5\% of the world's population user Instagram.
% A quarter of these users are based inside the United States, meaning that nearly 1 in 3 United States citizens uses Instagram~\cite{igstats}.
% Beyond its scale, Instagram is interesting for its content.
% Photographs are extremely rich, capturing information on all sorts of human activities and interactions.
% Although images can be more difficult to analyze than text, the research community has begun to study Instagram behavior~\cite{hu2014we, bakhshi2014faces} and even selifes~\cite{souza2015dawn}.

% In this paper, we show that a popular face recognition API can be used to scalably and accurately learn the gender and race of Instagram users.
% We use a dataset of 200 Instagram profiles, labeled for gender and race, to analyze the practicality and accuracy of facial recognition, achieveing 86\% accuracy for gender and 82\% accuracy for race in a limited setting.
% We additionally explore the accuracy of labeling different demographics as the amount of data increases, showing some concerns about algorithmic bias.
% We conclude with ideas for future work.

% \textbf{Data} \\
% \textbf{Collection} \\
% We used a subset of the Instagram data collected in~\cite{riederer2015cosn}.
% In this paper, the authors gathered the metadata (such as time of photo, URL of image, tags, location, etc.) for all photographs of a ``root" user, Kevin Systrom, the founder of Instagram.
% They then collected the user IDs of users who had commented or liked his photos, gathered their metadata, and repeated this process.
% A subset of these users were then selected based on geography-- only users with more than half of their photographs taken in Los Angeles or New York were kept.

% Two research assistants labeled a randomly selected subset of 200 of these profiles for gender and race.
% After filtering for private, deleted, or business profiles, 172 profiles remained.
% For gender, the labelers selected from male, female, or other.
% In practice, only the male or female categories ended up being used.
% For race, a subset of the United States Census categories were used: White, Black, Hispanic, Asian, and other.
% The labelers agreed on gender for 170 of the profiles and for race on 147.
% The process resulted in 76 profiles as Male and 94 as Female.
% For race, 75 were labeled White, 28 as Hispanic, 27 as Black, 16 as Asian, and 1 as other.

% Our next step was to recognize and label faces present in these Instagram users' profiles using computer vision.
% For this task, we used Face++~\cite{faceplusplus}, a popular API with reported high degress of accuracy~\cite{bakhshi2014faces}.
% In addition to recognizing faces within images, Face++ labels race from {White, Black, Asian} with a \emph{confidence score} 0-100, gender from {Male, Female} with a \emph{confidence score} 0-100.

% For each user in this data set, we gathered the metadata of the first 100 Instagram photos.
% Each image was then analyzed with the Face++ API.
% Face++ only requires that a URL to an image is passed to it.
% Therefore, this methodology does \emph{not} require that any images are downloaded, uploaded, or even viewed by a human labeler.
% Note that not every photo on Instagram has a face in it, and some have more than one.

% This resulted in 170 distinct users with at least one or more face present in their photos.
% We obtained 5,272 photos and depicting 12,143 faces. 
% Additionally, for each user, we passed the URLs for their profile pictures.
% A total of 70 users had profile pictures in which Face++ could detect a face.
% 73 faces were found: 2 profiles pictures had 2 faces in them.

% \textbf{Description}


% \begin{figure}[t]
%   \centering
%   \begin{subfigure}[b]{.21\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/census/face_cdf_gender.eps}
%     \caption{}
%     \label{fig:face_cdf_gender}
%   \end{subfigure}
%   \begin{subfigure}[b]{.21\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/census/face_cdf_race.eps}
%     \caption{}
%     \label{fig:face_cdf_race}
%   \end{subfigure}
%   \caption{}
%   \label{fig:face_cdf}
% \end{figure}

% \begin{figure}[t]
%   \centering
%   \begin{subfigure}[b]{.21\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/census/numphoto_cdf_gender.eps}
%     \caption{}
%     \label{fig:numphoto_cdf_gender}
%   \end{subfigure}
%   \begin{subfigure}[b]{.21\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/census/numphoto_cdf_race.eps}
%     \caption{}
%     \label{fig:numphoto_cdf_race}
%   \end{subfigure}
%   \caption{}
%   \label{fig:numphoto_cdf}
% \end{figure}


% \begin{figure}[t]
%   \centering
%   \begin{subfigure}[b]{.21\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/census/followers_cdf_gender.eps}
%     \caption{}
%     \label{fig:followers_cdf_gender}
%   \end{subfigure}
%   \begin{subfigure}[b]{.21\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/census/followers_cdf_race.eps}
%     \caption{}
%     \label{fig:followers_cdf_race}
%   \end{subfigure}

%   \begin{subfigure}[b]{.21\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/census/following_cdf_gender.eps}
%     \caption{}
%     \label{fig:following_cdf_gender}
%   \end{subfigure}
%   \begin{subfigure}[b]{.21\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/census/following_cdf_race.eps}
%     \caption{}
%     \label{fig:following_cdf_race}
%   \end{subfigure}
%   \caption{}
%   \label{fig:follow_cdf}
% \end{figure}

% We next compared several Instagram behaviors by demographic.
% Figure~\ref{fig:face_cdf_gender} shows that women in our dataset typically have more faces in their photographs than men.
% There appears to be more parity in number of faces for each of the considered racial groups, with Hispanics and Asians displaying slightly more faces in their photos.

% We examine potential differences in the total number of photos posted by a user to their account in Figure~\ref{fig:numphoto_cdf}.
% There does not appear to be a great difference for any particular group.

% We investigate the relationship between gender, race, and following behavior in Figure~\ref{fig:follow_cdf} .
% Instagram has a directional follower relationship, akin to that of Twitter.
% If a user, Alice, follows Bob, it means that Alice will see updates from Bob in her feed.
% Bob, however, will not see updates from Alice unless he decides to follow her.
% Figures~\ref{fig:followers_cdf_gender} and~\ref{fig:following_cdf_gender} show differences in following behavior within our dataset for gender. 
% It appears that men have more followers, and follow slightly more people.

% In Figures~\ref{fig:followers_cdf_race} and~\ref{fig:following_cdf_race}, we see can make a few observations for following behavior among race in our dataset.
% First, we see that the upper 50\% of Black users in our dataset have more followers than other groups, except for at the very top percentiles, where White users dominate.
% At the highest percentiles, Black users \emph{follow} the most people.
% % TODO: mention Black Twitter...
% % http://www.slate.com/articles/technology/technology/2010/08/how_black_people_use_twitter.html
% % Maybe http://dl.acm.org/citation.cfm?id=1963503
% % duggan and smith: demo of key social networking platforms



% \textbf{Methodology}
% % Recognizing and labeling the faces present in each photo does not definitively tell us the gender or race of the user profile. 
% The face recognition software only works at the level of a single photograph.
% % This does not tell us the race or gender of the user on which these photographs appear.
% We thus need to use an algorithm to go from the data of each picture to labeling an entire profile.
% We rely on one main assumption: the owner of profile will appear in more photos than any other individual.

% This assumption led us to test several different algorithms: 
% \begin{itemize}
% \item \textbf{Majority rule}: Count each face as a vote. 
%   Profile is labeled with the gender (or race) with the most votes.
% \item \textbf{Weighted majority rule}: Count each face as a vote.
%   However, a face now gets as many votes as the confidence score.
%   Thus, faces with lower confidence get lower weight.
%   Profile is labeled with the gender (or race) with the most votes.
% \item \textbf{Profile picture}: We simply take the result of the profile picture labeling.
% \end{itemize}

% Additionally, we applied a ``face weight" correction to all of these.
% In a photo with three faces, only one of these faces could be the user.
% It therefore might make sense to weight each face in this photo lower than in a photo with one face.
% The face weight (fw) correction does just this, multiplying the weight of photo by the inverse of the number of faces in that photograph.
% In our original example, the weight of each of the three faces would be multiplied by one third.
% This is equivalent to averaging the gender or race (potentially with confidence scores) of all users in a photograph.


% \textbf{Results}

% \textbf{Gender}
% Our human labelers categorized 76 profiles as Male and 94 as Female.
% Running our three algorithms with and without the face weight correction, we obtained the following results:
% \begin{itemize}
% \item \textbf{Majority rule}: 85.1\%
% \item \textbf{Weighted majority rule}: 85.7\%
% \item \textbf{Majority Rule, FW}: 86.9\%
% \item \textbf{Weighted Majority Rule, FW}: 87.5\%
% \item \textbf{Profile picture}: 86.3\%
% \end{itemize}

% We observe that using the face weight correction improves the results for both algorithms.
% Although profile pictures provide accurate information, only 70 out of 170 of these users had a profile picture that included a face.
% For the remainder of this section, we will focus on the highest accuracy algorithm, weighted majority rule with FW.
% A feature of this algorithm is that it outputs a probability for each user, enabling us to analyze the performance in more detail.
% % \begin{figure}[!tbp]
% %   \centering
% %   \begin{minipage}[b]{0.4\textwidth}
% %     \includegraphics[width=\textwidth]{flower1.jpg}
% %     \caption{Flower one.}
% %   \end{minipage}
% %   \hfill
% %   \begin{minipage}[b]{0.4\textwidth}
% %     \includegraphics[width=\textwidth]{flower2.jpg}
% %     \caption{Flower two.}
% %   \end{minipage}
% % \end{figure}

% \begin{figure}[h]
%   \centering
%   \begin{minipage}{.21\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/census/accuracy_gender.eps}
%     \caption{Predicted accuracy versus actual accuracy }
%     \label{fig:accuracy_gender}
%   \end{minipage}
%   \begin{minipage}{.21\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/census/roc_gender.eps}
%     \caption{True positive versus false positive rate when labeling at various threshold levels.}
%     \label{fig:roc_gender}
%   \end{minipage}
%   \caption{}
%   \label{fig:accuracy_gender_all}
% \end{figure}

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=.5\linewidth]{fig/census/facebin_gender.eps}
%   \caption{Number of faces detected in first 100 photos vs. accuracy, by gender}
%   \label{fig:facebin_gender}
% \end{figure}
% % TODO: add the boxplot thing in here, just because this looks bad right now

% In Fig.~\ref{fig:accuracy_gender}, we group individuals on how what our algorithm predicts is the likelihood that that individual is female, and plot the accuracy within each group.
% This shows us how well calibrated our algorithm is; if the output probability estimates are perfectly accurate, then half of the users with estimate 50\% should be female, and this plot should align on the 1:1 line.
% Instead, we see that the line is above the diagonal, meaning our accuracy estimate is actually an underestimate.
% Most likely, we could incorporate a prior probability into our algorithm to make it better calibrated.

% In Fig.~\ref{fig:roc_gender}, we plot an ROC curve, showing the trade-off between accurate and inaccurate labelings when using a threshold on the algorithm's output probability.
% For example, if we label as female all users with a probability of female over 60\%, around 75\% of female users would be correctly labeled, and we would exclude properly all but around 7\% of males.

% In Fig.~\ref{fig:facebin_gender}, we observe that for users labeled female, as more faces are detected in their profiles, accuracy increases.
% Perhaps counterintuitively, we see a dip for men, where male users with the 40-80\% most faces have much lower accuracy those in the bottom two quintiles, the bottom 40\%.
% One possible hypothesis to explain this is that as some users add more faces, they will start to add a larger diversity of faces.
% When this diversity increases, accuracy may decrease.
% We leave the question of whether this is the mechanism open for later work.


% \textbf{Race} \\

% For race, 75 users were labeled White, 28 as Hispanic, 27 as Black, 16 as Asian, and 1 as other.
% However, Face++ only labels users as Asian, Black, or White and will therefore always be incorrect on any of our users categorized as Hispanic or ``Other".
% Thus, we'll present results both for all users, and for the reduced set of users labeled manually by our research assistants as Asian, Black, or White (``filtered").
% Running our algorithms with and without the face weight correction, we obtained the following results:
% \begin{itemize}
% \item \textbf{Majority rule}: 64.4\% (all) 79.3\% (filtered)
% \item \textbf{Weighted majority rule}: 66.2\% (all) 82.1\% (filtered)
% \item \textbf{Majority Rule, FW}: 66.4\% 82.2\% (filtered)
% \item \textbf{Weighted Majority Rule, FW}: 66.2\% (all), 82.1\% (filtered)
% \item \textbf{Profile picture}: 57.1\% (all), 71.1\% (filtered)
% \end{itemize}

% On the users for which we have some hope of accuracy, the best algorithm achieves 82.2\% accuracy.
% For the remainder of this section, we will constrain our results to the Weighted Majority algorithm, FW, due to the probabilities that it outputs and its nearly identital performance to the next best algorithm.
% Additionally, we will look at some labelings as a binary classification problem between White users and Minority users.

% % TODO: fix titles here
% \begin{figure}[h]
%   \centering
%   \begin{minipage}{.21\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/census/accuracy_race_all.eps}
%     % \caption{Predicted accuracy versus actual accuracy }
%     \label{fig:accuracy_race_all}
%   \end{minipage}
%   \begin{minipage}{.21\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/census/accuracy_race_filtered.eps}
%     % \caption{True positive versus false positive rate when labeling at various threshold levels.}
%     \label{fig:accuracy_race_filtered}
%   \end{minipage}
%   % \caption{}
%   \label{fig:accuracy_race}
% \end{figure}

% \begin{figure}[h]
%   \centering
%   \begin{minipage}{.21\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/census/roc_race.eps}
%     % \caption{Predicted accuracy versus actual accuracy }
%     \label{fig:roc_race}
%   \end{minipage}
%   \begin{minipage}{.21\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/census/facebin_race_filtered.eps}
%     % \caption{True positive versus false positive rate when labeling at various threshold levels.}
%     \label{fig:facebin_race_filtered}
%   \end{minipage}
%   \caption{}
%   \label{fig:more_race}
% \end{figure}

% Based on Fig.~\ref{fig:accuracy_race}, the algorithm does not appear to be well-calibrated, both in being overconfident in the users with low probability estimations, and being underconfident in users with higher estimates.

% An important aspect of demographic labeling is considering issues of the digital divide or disparate impact.
% In Fig.~\ref{fig:more_race}, we see that accuracy is much lower on minorities than it is on White users.
% Again, we see a lowering of accuracy as the number of faces increases, akin to the dip in accuracy Male users show in Fig.~\ref{fig:facebin_gender}.

% \textbf{Discussion} \\
% Among our dataset, we see some examples of differences in behavior.
% For example, women tended to have more faces in their photos, and black users tended to have more followers.
% A larger sample and careful statistical analysis should be taken to verify these results.
% Applying machine learning to many more profiles could reveal, and possibly help explain, these and other differences in behaviors on Instagram.

% In applying these techniques there are dangers of algorithmic bias.
% The large difference in accuracy between white users and minority users is an example of this.
% Our technique as it stands could suffer from this issue.
% For example, users with more diverse faces in their profile (both gender and racially) may be harder to label.
% Using a thresholding technique to only obtian high-accuracy users might leave only users who display strong homophily.

% Further exascerbating the problem is that racial minority users actually \emph{decrease} in accuracy as there is more data about them, up to a certain point. 
% Clearly more work is need to be done to understand this issue.
% It may be important to consider the trade-offs here.
% A more accurate algorithm may not be preferable to an algorithm that is equally accurate across demographic groups, and that behaves similarly in regards to the scaling of data.

% % \section{Conclusion}
% \textbf{Conclusion} \\
% In this work, we've shown that computational vision techniques have some promise in becoming a valuable tool for demographers.
% By combining facial recognition with the OSN Instagram, we've proven that the race and gender of users can be inferred with high precision and recall.

% We see several important next steps to this work.
% First, a larger scale verification of the results of this work should be obtained, with more users, aiming for diversity in many senses-- culturally, economically, racially, geographically, etc.
% Such a verification should investigate the accuracy of the technique on various demographics in order to minimize algorithmic bias. 

% Another direction is to use more powerful machine learning techniques on this problem.
% For example, instead of naively incorporating all faces in a profile, a researcher could cluster faces based on a similarity score.
% The largest cluster would most likely contain the users face.
% Other facial recognition software packages, with a wider range of races, or with other features, could improve upon these results.

% Finally, this technique could be used to engage in studies of various demographic groups and answer different questions.
% Do different demographics use social networks in different ways?
% What can we learn about interactiokn on the OSN between groups?
% Combining demographic data with location data, we could additionally learn about immigration or human mobility.
